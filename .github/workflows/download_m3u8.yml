name: Video Stream Recorder

on:
  repository_dispatch:
    types: [download-m3u8]
    
jobs:
  download:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Install FFmpeg
        run: |
          sudo apt-get update
          sudo apt-get install -y ffmpeg
        
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install yt-dlp
          
      - name: Generate and verify YouTube cookies
        id: cookies
        run: |
          # Create cookie jar file
          touch youtube_cookies.txt
          
          # Define browser-like headers
          HEADERS=(
            -H "User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
            -H "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8"
            -H "Accept-Language: en-US,en;q=0.9"
            -H "Accept-Encoding: gzip, deflate, br"
            -H "Connection: keep-alive"
            -H "Upgrade-Insecure-Requests: 1"
            -H "Sec-Fetch-Dest: document"
            -H "Sec-Fetch-Mode: navigate"
            -H "Sec-Fetch-Site: none"
            -H "Sec-Fetch-User: ?1"
            -H "Cache-Control: max-age=0"
            -H "TE: trailers"
          )
          
          # Fetch multiple YouTube pages to get a complete set of cookies
          echo "Fetching YouTube pages to generate cookies..."
          curl -s -c youtube_cookies.txt -b youtube_cookies.txt "${HEADERS[@]}" "https://www.youtube.com/"
          curl -s -c youtube_cookies.txt -b youtube_cookies.txt "${HEADERS[@]}" "https://www.youtube.com/feed/trending"
          curl -s -c youtube_cookies.txt -b youtube_cookies.txt "${HEADERS[@]}" "https://www.youtube.com/feed/subscriptions"
          
          # Check if cookies file has content
          if [ -s youtube_cookies.txt ]; then
            # Convert curl cookies to Netscape format that yt-dlp can use
            echo "# Netscape HTTP Cookie File" > yt_cookies.txt
            grep -v "^#" youtube_cookies.txt | grep -v "^$" | \
            awk '{print $1 "\t" ($2 == "" ? "FALSE" : "TRUE") "\t" $3 "\t" ($4 == "" ? "FALSE" : "TRUE") "\t" $5 "\t" $6 "\t" $7}' >> yt_cookies.txt
            
            # Verify that important cookies exist
            if grep -q "CONSENT" yt_cookies.txt && grep -q "VISITOR_INFO1_LIVE" yt_cookies.txt; then
              echo "cookies_generated=true" >> $GITHUB_OUTPUT
              echo "Cookies generated and verified successfully"
            else
              echo "cookies_generated=false" >> $GITHUB_OUTPUT
              echo "Cookies missing important values"
            fi
          else
            echo "cookies_generated=false" >> $GITHUB_OUTPUT
            echo "Failed to generate cookies"
          fi
          
          # Display the cookies for debugging
          echo "Cookie contents:"
          cat yt_cookies.txt
          
      - name: Try alternative method with yt-dlp extractors
        id: generate_config
        if: steps.cookies.outputs.cookies_generated != 'true'
        run: |
          # Create .netrc file for authentication
          echo "machine youtube.com login none password none" > ~/.netrc
          chmod 600 ~/.netrc
          
          # Create yt-dlp config with optimized settings
          mkdir -p ~/.config/yt-dlp/
          cat > ~/.config/yt-dlp/config << EOF
          --user-agent "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
          --add-header "Accept-Language:en-US,en;q=0.9"
          --add-header "Origin:https://www.youtube.com"
          --add-header "Referer:https://www.youtube.com/"
          --mark-watched
          --embed-metadata
          --embed-chapters
          --embed-thumbnail
          --embed-subs
          --extractor-args "youtube:player_client=android,web"
          --extractor-retries 5
          --ignore-errors
          --no-check-certificates
          EOF
          
          echo "config_created=true" >> $GITHUB_OUTPUT
          
      - name: Download stream
        id: download
        run: |
          mkdir -p downloads
          
          # Check if this is a YouTube URL or M3U8
          IS_YOUTUBE="${{ github.event.client_payload.is_youtube }}"
          URL="${{ github.event.client_payload.url }}"
          
          if [ "$IS_YOUTUBE" = "true" ]; then
            echo "Downloading YouTube video..."
            
            # Define advanced YouTube options
            YT_OPTS="--force-ipv4 --sleep-interval 2 --max-sleep-interval 5 --geo-bypass"
            
            # Check if we should use cookies
            if [ "${{ steps.cookies.outputs.cookies_generated }}" = "true" ]; then
              YT_OPTS="$YT_OPTS --cookies yt_cookies.txt"
              echo "Using verified cookies"
            else
              echo "No valid cookies available, using alternative options"
            fi
            
            if [ "${{ github.event.client_payload.is_live }}" = "true" ]; then
              echo "Using live stream options..."
              yt-dlp \
                $YT_OPTS \
                --live-from-start \
                --wait-for-video 10-30 \
                --retries 10 \
                --user-agent "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" \
                --add-header "Accept-Language:en-US,en;q=0.9" \
                --extractor-args "youtube:player_client=android,web" \
                -o "downloads/%(title)s.%(ext)s" \
                "$URL" || \
              # Fall back to alternative URL format if standard one fails
              yt-dlp \
                $YT_OPTS \
                --live-from-start \
                --wait-for-video 10-30 \
                --retries 10 \
                --user-agent "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" \
                --add-header "Accept-Language:en-US,en;q=0.9" \
                --extractor-args "youtube:player_client=android,web" \
                -o "downloads/%(title)s.%(ext)s" \
                "https://www.youtube.com/watch?v=$(echo $URL | grep -oP '(?<=youtu.be/|v=|/v/|youtube.com/embed/|youtube.com/shorts/)[^&?]*' || echo $URL)"
            else
              echo "Using standard YouTube download..."
              yt-dlp \
                $YT_OPTS \
                --user-agent "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" \
                --add-header "Accept-Language:en-US,en;q=0.9" \
                --extractor-args "youtube:player_client=android,web" \
                -o "downloads/%(title)s.%(ext)s" \
                "$URL" || \
              # Fall back to alternative URL format if standard one fails
              yt-dlp \
                $YT_OPTS \
                --user-agent "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" \
                --add-header "Accept-Language:en-US,en;q=0.9" \
                --extractor-args "youtube:player_client=android,web" \
                -o "downloads/%(title)s.%(ext)s" \
                "https://www.youtube.com/watch?v=$(echo $URL | grep -oP '(?<=youtu.be/|v=|/v/|youtube.com/embed/|youtube.com/shorts/)[^&?]*' || echo $URL)"
            fi
          else
            echo "Downloading M3U8 stream..."
            yt-dlp \
              -o "downloads/%(title)s.%(ext)s" \
              "$URL"
          fi
          
          # List downloaded files
          echo "Downloaded files:"
          ls -la downloads/
          
          # Set output for email notification
          if [ "$(ls -A downloads 2>/dev/null)" ]; then
            ORIGINAL_FILENAME=$(ls downloads | head -n 1)
            echo "download_name=$ORIGINAL_FILENAME" >> $GITHUB_OUTPUT
            echo "Original filename: $ORIGINAL_FILENAME"
          else
            echo "No files were downloaded"
            exit 1
          fi
          
      - name: Process large files
        id: process
        run: |
          mkdir -p processed
          FILENAME=$(ls downloads | head -n 1)
          FILEPATH="downloads/$FILENAME"
          
          # Get file size in bytes
          FILE_SIZE=$(stat -c%s "$FILEPATH")
          echo "File size: $FILE_SIZE bytes"
          
          # 3.8GB in bytes (leaving some margin below 4GB)
          MAX_SIZE=4080218931
          
          if [ "$FILE_SIZE" -gt "$MAX_SIZE" ]; then
            echo "File exceeds 3.8GB, processing..."
            
            # Get file extension
            EXTENSION="${FILENAME##*.}"
            BASENAME="${FILENAME%.*}"
            
            # Split into 3.8GB chunks using ffmpeg
            ffmpeg -i "$FILEPATH" -c copy -map 0 -segment_time 3800 -f segment "processed/${BASENAME}_%03d.$EXTENSION"
            
            # Create info file listing all parts
            echo "This video was split into multiple parts due to file size limitations:" > processed/README.txt
            ls processed/ | grep -v "README.txt" >> processed/README.txt
            
            echo "parts_created=true" >> $GITHUB_OUTPUT
            echo "Files after splitting:"
            ls -la processed/
          else
            echo "File is under size limit, no processing needed"
            cp "$FILEPATH" processed/
            echo "parts_created=false" >> $GITHUB_OUTPUT
          fi
          
      - name: Install jq
        run: sudo apt-get install -y jq
          
      - name: Upload to file.io
        id: upload
        run: |
          # Create array to hold all download URLs
          DOWNLOAD_URLS=()
          
          # Upload each file
          for FILE in processed/*; do
            if [ -f "$FILE" ]; then
              echo "Uploading $FILE..."
              RESPONSE=$(curl -F "file=@$FILE" https://file.io/?expires=14d)
              URL=$(echo $RESPONSE | jq -r '.link')
              FILENAME=$(basename "$FILE")
              
              if [ -n "$URL" ]; then
                echo "$FILENAME: $URL" >> download_links.txt
                DOWNLOAD_URLS+=("$FILENAME: $URL")
                echo "Uploaded $FILENAME to $URL"
              else
                echo "Failed to upload $FILENAME"
              fi
            fi
          done
          
          # Save all download links to output
          echo "download_links=$(cat download_links.txt | tr '\n' ',' | sed 's/,$//')" >> $GITHUB_OUTPUT
          
      - name: Send email notification
        if: success()
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: smtp.gmail.com
          server_port: 465
          username: ${{ secrets.EMAIL_USERNAME }}
          password: ${{ secrets.EMAIL_PASSWORD }}
          subject: Video Download Complete - ${{ github.event.client_payload.name || 'Video Recording' }}
          body: |
            Your video download has been completed successfully!
            
            Download Details:
            - Name: ${{ github.event.client_payload.name || 'Video Recording' }}
            - Original File: ${{ steps.download.outputs.download_name }}
            - URL: ${{ github.event.client_payload.url }}
            - Type: ${{ github.event.client_payload.is_youtube == 'true' && 'YouTube' || 'M3U8 Stream' }}
            
            ${{ steps.process.outputs.parts_created == 'true' && 'This video was split into multiple parts due to size limitations. Please download all parts:' || 'Download Link:' }}
            ${{ steps.process.outputs.parts_created == 'true' && steps.upload.outputs.download_links || steps.upload.outputs.download_url }}
            
            Note: Links will expire in 14 days. Files will be automatically deleted after download.
            
            Workflow Run: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
            
            This is an automated email from your Video Recorder application.
          to: ${{ github.event.client_payload.email }}
          from: Video Recorder <${{ secrets.EMAIL_USERNAME }}> 
