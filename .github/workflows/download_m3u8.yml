name: Video Stream Recorder

on:
  repository_dispatch:
    types: [download-m3u8]
    
jobs:
  download:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Install FFmpeg
        run: |
          sudo apt-get update
          sudo apt-get install -y ffmpeg
        
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install yt-dlp
          
      - name: Download stream
        id: download
        run: |
          mkdir -p downloads
          
          # Check URL type
          IS_YOUTUBE="${{ github.event.client_payload.is_youtube }}"
          IS_TWITTER="${{ github.event.client_payload.is_twitter }}"
          URL="${{ github.event.client_payload.url }}"
          
          if [ "$IS_YOUTUBE" = "true" ]; then
            echo "Downloading YouTube video via Cloudflare Worker..."
            
            # Set up common options for YouTube
            YT_OPTS="--force-ipv4 --retries 10 --geo-bypass"
            
            # Extract video ID
            VIDEO_ID=$(echo $URL | grep -oP '(?<=youtu.be/|v=|/v/|youtube.com/embed/|youtube.com/shorts/)[^&?]*' || echo $URL)
            
            # Use your Cloudflare Worker URL
            WORKER_URL="https://your-worker.your-subdomain.workers.dev/?v=$VIDEO_ID"
            
            yt-dlp \
              $YT_OPTS \
              --user-agent "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" \
              --add-header "Accept-Language:en-US,en;q=0.9" \
              -o "downloads/%(title)s.%(ext)s" \
              "$WORKER_URL"
          elif [ "$IS_TWITTER" = "true" ]; then
            echo "Downloading Twitter/X video..."
            
            # Set up Twitter download options
            TWITTER_OPTS="--force-ipv4 --retries 10 --cookies-from-browser chrome"
            
            yt-dlp \
              $TWITTER_OPTS \
              --user-agent "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" \
              --add-header "Accept-Language:en-US,en;q=0.9" \
              -o "downloads/twitter_%(id)s.%(ext)s" \
              "$URL"
          else
            echo "Downloading M3U8 stream..."
            yt-dlp \
              -o "downloads/%(title)s.%(ext)s" \
              "$URL"
          fi
          
          # List downloaded files
          echo "Downloaded files:"
          ls -la downloads/
          
          # Set output for email notification
          if [ "$(ls -A downloads 2>/dev/null)" ]; then
            ORIGINAL_FILENAME=$(ls downloads | head -n 1)
            echo "download_name=$ORIGINAL_FILENAME" >> $GITHUB_OUTPUT
            echo "Original filename: $ORIGINAL_FILENAME"
          else
            echo "No files were downloaded"
            exit 1
          fi
          
      - name: Process large files
        id: process
        run: |
          mkdir -p processed
          FILENAME=$(ls downloads | head -n 1)
          FILEPATH="downloads/$FILENAME"
          
          # Get file size in bytes
          FILE_SIZE=$(stat -c%s "$FILEPATH")
          echo "File size: $FILE_SIZE bytes"
          
          # 3.8GB in bytes (leaving some margin below 4GB)
          MAX_SIZE=4080218931
          
          if [ "$FILE_SIZE" -gt "$MAX_SIZE" ]; then
            echo "File exceeds 3.8GB, processing..."
            
            # Get file extension
            EXTENSION="${FILENAME##*.}"
            BASENAME="${FILENAME%.*}"
            
            # Split into 3.8GB chunks using ffmpeg
            ffmpeg -i "$FILEPATH" -c copy -map 0 -segment_time 3800 -f segment "processed/${BASENAME}_%03d.$EXTENSION"
            
            # Create info file listing all parts
            echo "This video was split into multiple parts due to file size limitations:" > processed/README.txt
            ls processed/ | grep -v "README.txt" >> processed/README.txt
            
            echo "parts_created=true" >> $GITHUB_OUTPUT
            echo "Files after splitting:"
            ls -la processed/
          else
            echo "File is under size limit, no processing needed"
            cp "$FILEPATH" processed/
            echo "parts_created=false" >> $GITHUB_OUTPUT
          fi
          
      - name: Install jq
        run: sudo apt-get install -y jq
          
      - name: Upload to file sharing service
        id: upload
        run: |
          # Create array to hold all download URLs
          DOWNLOAD_URLS=()
          UPLOAD_SUCCESS=true
          
          # Upload each file
          for FILE in processed/*; do
            if [ -f "$FILE" ]; then
              FILENAME=$(basename "$FILE")
              echo "Uploading $FILE..."
              
              # Get file size in MB
              FILE_SIZE_MB=$(du -m "$FILE" | cut -f1)
              echo "File size: $FILE_SIZE_MB MB"
              
              # Try upload with gofile.io as primary service
              UPLOAD_SUCCEEDED=false
              
              # First try gofile.io (no size limit)
              echo "Uploading to gofile.io..."
              RESPONSE=$(curl -F "file=@$FILE" https://store1.gofile.io/uploadFile)
              echo "Response: $RESPONSE"
              
              # Parse response safely
              if echo "$RESPONSE" | jq -e '.status' >/dev/null 2>&1 && [ "$(echo "$RESPONSE" | jq -r '.status')" = "ok" ]; then
                URL=$(echo "$RESPONSE" | jq -r '.data.downloadPage')
                
                if [ -n "$URL" ]; then
                  echo "$FILENAME: $URL" >> download_links.txt
                  DOWNLOAD_URLS+=("$FILENAME: $URL")
                  echo "Uploaded $FILENAME to $URL"
                  UPLOAD_SUCCEEDED=true
                fi
              fi
              
              # If gofile.io failed and file is small enough, try file.io as fallback
              if [ "$UPLOAD_SUCCEEDED" = "false" ] && [ "$FILE_SIZE_MB" -le 2000 ]; then
                echo "Gofile.io upload failed, trying file.io for fallback..."
                RESPONSE=$(curl -F "file=@$FILE" https://file.io/?expires=14d)
                echo "Response: $RESPONSE"
                
                # Parse response carefully
                if echo "$RESPONSE" | jq -e '.success' >/dev/null 2>&1 && [ "$(echo "$RESPONSE" | jq -r '.success')" = "true" ]; then
                  URL=$(echo "$RESPONSE" | jq -r '.link')
                  
                  if [ -n "$URL" ]; then
                    echo "$FILENAME: $URL" >> download_links.txt
                    DOWNLOAD_URLS+=("$FILENAME: $URL")
                    echo "Uploaded $FILENAME to $URL (via file.io)"
                    UPLOAD_SUCCEEDED=true
                  fi
                fi
              fi
              
              # If both services failed, mark for artifact upload
              if [ "$UPLOAD_SUCCEEDED" = "false" ]; then
                echo "Failed to upload $FILENAME to any service"
                echo "$FILENAME: Upload failed - will be available as GitHub artifact" >> download_links.txt
                UPLOAD_SUCCESS=false
              fi
            fi
          done
          
          # Create download_links.txt if it doesn't exist
          touch download_links.txt
          
          # Save all download links to output
          echo "download_links=$(cat download_links.txt | tr '\n' ',' | sed 's/,$//')" >> $GITHUB_OUTPUT
          
          # Set flag for artifact upload
          echo "upload_success=$UPLOAD_SUCCESS" >> $GITHUB_OUTPUT

      - name: Save files as artifacts (fallback)
        if: steps.upload.outputs.upload_success == 'false'
        uses: actions/upload-artifact@v3
        with:
          name: video-files
          path: processed/
          retention-days: 7
          
      - name: Update email message
        id: email_message
        run: |
          if [ "${{ steps.upload.outputs.upload_success }}" = "false" ]; then
            echo "message=Some files couldn't be uploaded to file sharing services and are available as artifacts in the GitHub workflow. You can download them from: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}" >> $GITHUB_OUTPUT
          else
            echo "message=Note: Links may expire after some time." >> $GITHUB_OUTPUT
          fi
          
      - name: Send email notification
        if: success()
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: smtp.gmail.com
          server_port: 465
          username: ${{ secrets.EMAIL_USERNAME }}
          password: ${{ secrets.EMAIL_PASSWORD }}
          subject: Video Download Complete - ${{ github.event.client_payload.name || 'Video Recording' }}
          body: |
            Your video download has been completed successfully!
            
            Download Details:
            - Name: ${{ github.event.client_payload.name || 'Video Recording' }}
            - Original File: ${{ steps.download.outputs.download_name }}
            - URL: ${{ github.event.client_payload.url }}
            - Type: ${{ github.event.client_payload.is_youtube == 'true' && 'YouTube' || github.event.client_payload.is_twitter == 'true' && 'Twitter/X' || 'M3U8 Stream' }}
            
            ${{ steps.process.outputs.parts_created == 'true' && 'This video was split into multiple parts due to size limitations. Please download all parts:' || 'Download Link:' }}
            ${{ steps.process.outputs.parts_created == 'true' && steps.upload.outputs.download_links || steps.upload.outputs.download_url }}
            
            ${{ steps.email_message.outputs.message }}
            
            Workflow Run: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
            
            This is an automated email from your Video Recorder application.
          to: ${{ github.event.client_payload.email }}
          from: Video Recorder <${{ secrets.EMAIL_USERNAME }}> 
          
      - name: Cleanup old workflow runs
        if: always()
        uses: Mattraks/delete-workflow-runs@v2
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          repository: ${{ github.repository }}
          retain_days: 7
          keep_minimum_runs: 10
          delete_workflow_pattern: download_m3u8.yml 
